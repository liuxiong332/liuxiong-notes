## Softmax分类器

#### 逻辑回归分类器

逻辑回归分类器是一种通过线性表达式结合Sigmoid函数生成概率的分类方法。其中预测值h`(x)`是通过如下公式求得：
$$
h(x)  = e ^ {1 + e^{-WX}}
$$
由于H(x)大小是[0,1]区间内，因此可以看成概况值，则最终Y可以写成：
$$
H(x) = h(x) ^ y (1 - h(x)) ^ {1 - y}
$$
根据我们的极大似然估计得出我们的损失函数为
$$
J(W) = \prod^{i<N}_{i=1}h(x_i)^{y_i}(1 - h(x_i)) ^ {1 - y_i}
$$
然后根据Log损失值，使用梯度下降算法，可以求得相应的W。

逻辑回归分类器只能用于二分类。



#### Softmax分类器

Softmax分类器是多分类器，它通过输出概率值来进行分类。他和单层神经网络结构相似，每个分类结果都通过如下的线性结果得出：
$$
h(x) = e ^ {WX}
$$
对于多分类$h_n(x)$，每个数据结果都可以用比率值计算
$$
L_i = \frac{e^{W_iX_i}}{\sum {e ^ {W_iX_i}}}
$$
 其中损失值为
$$
Loss = -LogLi = -Log\frac{e^{W_IX_I}}{\sum {e^{W_IX_I}}}
$$
计算的时候需要使用BP（Back Propagation）反向传播进行调节参数。

其中
$$
\frac{\partial Loss}{\partial {f_{y_i}}} = P_{f_{y_i}} - 1
$$
最后结果的形式非常的简单，只要将算出来的概率的向量对应的真正结果的那一维减1，就可以了

举个例子，通过若干层的计算，最后得到的某个训练样本的向量的分数是[ 1, 5, 3 ], 那么概率分别就是[0.015,0.866,0.117],如果这个样本正确的分类是第二个的话，那么计算出来的偏导就是[0.015,0.866−1,0.117]=[0.015,−0.134,0.117]，是不是很简单！！然后再根据这个进行back propagation就可以了