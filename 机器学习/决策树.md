## 决策树分类

决策树模型（Decision Tree）是一个模拟人类决策过程思想的模型，它是一种简单，逻辑清楚，可解释性好的分类算法。

####  ID3算法

信息熵是度量样本集合纯度最常用的一种指标，假定当前集合D中第k类样本所占比例为$p_k$，则信息熵定义为：

$$
Ent(D)  = -\sum_{k=1}^{|y|}p_klogp_k
$$
$Ent(D)$的值越小，则D的纯度越高。信息增益以信息熵为基础，计算当前划分对信息熵所造成的变化。

信息增益定义如下：

$$
Gain(D) = Ent(D) - \sum_{v=1}^{v}\frac{|D_v|}{|D|}Ent(D_k)
$$
即对于属性v的信息增益就是划分前的信息熵减去划分后的信息熵和。

ID3算法的核心就是每次进行分类决策的时候，都对每个分类属性求出信息增益Gain，信息增益最高的则为划分属性。例如对于属性$a_1, a_2, a_3, ...  a_n$，我们分别求出各自的信息增益$Gain(a1)，Gain(a2)，Gain(a3)，...，Gain(a_n)$，于是划分属性$a = arg max(Gain(a_1), Gain(a_2), ... , Gain(a_n))$。

#### C4.5

ID3算法对于取值较多的属性有所偏好，例如对于很多数据有的ID属性，它本身没啥意义，但是如果使用ID3进行分类，那么ID这个属性的信息增益将会很高，这本身是一种不合理。

C4.5使用信息增益率来规避这种缺陷。

$$
Gain_ratio(D, a) = \frac{Gain(D, a)}{IV(a)}
$$
其中$IV(a) = -\sum_{v=1}^{V}\frac{D_v}{D}log\frac{D_v}{D}$

属性a的可能取值数目越多，即V越大，那么$IV(a)$取值通常越大。

#### CART

CART算法使用Gini指数来评估划分的指标。

$$
Gini(D) = \sum_{k=1}^{|y|}\sum_{k'\ne k}p_kp_{k'} = 1 - \sum_{k=1}^{|y|}p_k^2
$$
Gini指数反映了随机选择两个样例，其列别标记不一样的概率。Gini系数越小，数据集D的纯度越高。

属性a的基尼系数：$Gini_index(D, a) = \sum_{v=1}^{V}\frac{D^v}{D}Gini(D^v)$

不同于ID4.5，CART算法使用Gini系数来决定使用哪个属性进行划分。即把基尼系数当作纯度划分指标。

#### 预剪枝和后剪枝

预剪枝是提前终止某些分支的生长，后剪枝是先生成一颗完全树，再回头剪枝。

最简单的预剪枝可以通过**限制树的深度，叶子节点的数目和信息熵的变化值**来避免过多的叶子节点，从而使树的深度和广度都限制在一定范围内，避免过拟合。

深层次的剪枝则需要使用评估方法来决策是否进行剪枝。

剪枝过程中需评估剪枝前后决策树的优劣。可以使用**“留出法”配合精度计算**来进行评估。即将训练数据分为训练集和验证集。

预剪枝的过程如下：

对于划分节点，若不划分，则将其标记为叶子节点，类别标记为样例中最多的列别。然后我们使用验证集来评估是否需要剪枝。如果不划分的验证集精度比划分时高，那么就剪掉此节点，不进行划分。

后剪枝过程如下：

首先通过决策树算法生成完成树，计算出完整决策树在验证集上的准确率。然后对于每个叶节点的父节点，计算若将其替换成叶子节点之后在验证集的准确率，如果验证集上精度提高，则执行剪枝。然后对于每个叶子节点的父节点执行剪枝操作，直到不能进行剪枝为止。

预剪枝会减少训练时间，降低过拟合风险，有可能会导致欠拟合。

后剪枝会增加训练时间，降低过拟合风险，欠拟合风险基本不变。后剪枝返回性能一般由于预剪枝。

#### 连续值处理

如果数据集中有连续值，我们需要将这些连续值进行离散化。n个属性值可形成n-1个获选划分。对于每个候选划分，我们就需要找到最佳划分来讲数据分成两堆。

对于最佳划分方式，我们可以像决策树决策方法一样，对每个划分使用信息熵增益或者Gini指数来获取最佳的划分方式，如此我们将得到一个划分值$a_v$，这样我们的决策树左右分支就可以表示为$v >= a_v$和$v < a_v$。

#### 缺失值处理

在我们的数据中，常常会遇到属性值缺失。如果我们把这些缺失数据的属性直接扔掉，会造成极大的浪费。

我们可以通过如下方法来处理。

1. 当计算信息熵或者Gini指数的时候，忽略缺失值的影响，即不考虑缺失值的分类。例如有17个样本数据，有3个数据在划分属性上没有数据，于是我们就可以假定只有14条样本数据，令这14条数据记作$D'$，计算其$Gain(D’, a)$或者$Gini(D‘, a)$。
2. 按照上述方法计算出来的熵增益会比正常的增益要高（纯度更高）。于是$Gain(D, a)  = \frac{|D'|}{|D|}Gain(D', a)$得出属性值的信息熵增益。