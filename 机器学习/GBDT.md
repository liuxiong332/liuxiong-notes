## GBDT

GBDT（Gradient Boosting Decision Tree）是一种增强型Boosting算法，它利用一些弱分类器组合来构造一个强分类器。GBDT是由多棵决策树构成，通常都是上百棵树，而且每棵树规模都较小（即树的深度会比较浅）。模型预测的时候，对于输入的每个样本实例，然后会遍历每一棵决策树，每棵树都会对预测值进行调整修正，最后得到预测的结果。

GBDT构造方法是首先使用一个弱分类器来进行初次建模，然后每次计算都是为了减少上一次的残杀。GB每一次的计算是为了减少上一次的残差，就可以在残差减少的梯度方向上建立一个新的模型。在Gradient Boost中，每个新的模型是为了使得之前模型的残差往梯度方向减少，与Boosting对错误的样本加大加权的做法有着很大的区别。

GBDT的每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习。如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄。如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学习。

GBDT的学习过程

ABCD四个人的年龄分别为15,19,23,27，分别为高中生、大学生、运动员和码农。
决策树学习过程：

![img](https://img-blog.csdn.net/20150913144457164?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

 

GBDT的学习过程：

![img](https://img-blog.csdn.net/20150913144554566?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

现在A,B,C,D的预测值都和真实年龄一致
A: 15岁高中学生，收入较少，天天没时间玩电脑；预测年龄A = 17– 2 = 15

B: 19岁大学生；收入较少，天天宅在宿舍玩电脑；预测年龄B = 17+ 2 = 19

C: 23岁运动员；收入较多，体育训练没时间玩电脑；预测年龄C = 25 – 2 = 23

D: 27岁码农；收入较多，长时间玩电脑；预测年龄D = 25 + 2 = 27